**Aim**  
Recreate one or more images provided to match exact proportions, composition, and resolution as closely as possible using any tool, software, or generative AI platforms. The given images can be seen here - [given image 1](https://github.com/itisha89/image_recreation/blob/main/Given_pic_1.jpg), [given image 2](https://github.com/itisha89/image_recreation/blob/main/Given_pic_2.jpg), [given image 3](https://github.com/itisha89/image_recreation/blob/main/Given_pic_3.jpg).

---

**Workflow**  

### 1️⃣ Initial Exploration of Models  
Available models: GANs, Variational Autoencoders (VAEs), Transformers, DreamBooth, StyleDrop, Diffusers (Stable Diffusion v1.5, Stable Diffusion XL (SDXL), Kandinsky 2.2), DALL-E, SDEdit, ControlNet, and Instruct-Pix2Pix.

### 2️⃣ Model Comparison & Shortlisting  
- **Training Data Requirement**: GANs (conditional GANs, GAN inversions), VAEs, and Transformers need training data, but we require an on-the-fly image recreation, so these models were excluded.  
- **Text-to-Image Models**: Since the captions generated by BLIP weren’t detailed enough, DALL-E was discarded. The captions can be seen in the [code file](https://github.com/itisha89/image_recreation/blob/main/Code.pdf).  
- **Pre-Trained Models Considered**: Diffusers (Stable Diffusion v1.5, SDXL, Kandinsky 2.2), SDEdit, ControlNet.  
- **Exclusion**: ControlNet is more suited for edge detection and sketch-to-image tasks and was therefore excluded.  
- **Best Performers**: On comparing Instruct-Pix2Pix, SDEdit, and ControlNet, Instruct-Pix2Pix outperformed the others.  
- **Shortlisted Models**: Diffusers (Stable Diffusion v1.5, SDXL, Kandinsky 2.2) and Instruct-Pix2Pix.  

### 3️⃣ Application & Fine-Tuning  
- **Fine-Tuning**: Diffusers were fine-tuned with `strength` and `guidance_scale`. Instruct-Pix2Pix performed well with a simple prompt, so no fine-tuning was needed.  
- **Exclusion**: Kandinsky 2.2 didn’t produce satisfactory results and was excluded. The outcomes can be seen here - [generated_image_kandinsky_1](https://github.com/itisha89/image_recreation/blob/main/gen_image_kandinsky_1.jpg), [generated_image_kandinsky_2](https://github.com/itisha89/image_recreation/blob/main/gen_image_kandinsky_2.jpg), [generated_image_kandinsky_3](https://github.com/itisha89/image_recreation/blob/main/gen_image_kandinsky_3.jpg).  
- **Final Models**: Stable Diffusion v1.5, SDXL, and Instruct-Pix2Pix were retained for further processing.  

### 4️⃣ Quality Assurance  
- The generated images and given images were compared on the basis of resolution, aspect ratio, mode, and channels. The initial results can be seen [here](https://github.com/itisha89/image_recreation/blob/main/Eval_before_preprocess_image_1.png).  
- **Issue**: Models didn’t generate correct resolutions for `given image 1`, likely due to random shifts in output dimensions and lack of preprocessing.  
- **Solution**: Resized `given image 1` to ensure a 1:1 aspect ratio and applied models again. The resized image can be seen [here](https://github.com/itisha89/image_recreation/blob/main/Given_pic_1_resized.jpg).  
- **Outcome**: After resizing, all models successfully recreated `given image 1` with the exact proportions, composition, and resolutions. The results after resizing can be seen [here](https://github.com/itisha89/image_recreation/blob/main/Eval_after_preprocess_image_1.png).  

### 5️⃣ Conclusion  
- **Findings**: Stable Diffusion v1.5 slightly distorted the image, while SDXL provided a smoother output with better structure. Instruct-Pix2Pix produced the most accurate replicas of the original image.  
- **Generated Outputs**:  
  - **Stable Diffusion v1.5**: [gen_image_sd_1.2.jpg](https://github.com/itisha89/image_recreation/blob/main/gen_image_sd_1.2.jpg), [gen_image_sd_2.jpg](https://github.com/itisha89/image_recreation/blob/main/gen_image_sd_2.jpg), [gen_image_sd_3.jpg](https://github.com/itisha89/image_recreation/blob/main/gen_image_sd_3.jpg)  
  - **Stable Diffusion XL**: [gen_image_sdxl_1.2.jpg](https://github.com/itisha89/image_recreation/blob/main/gen_image_sdxl_1.2.jpg), [gen_image_sdxl_2.jpg](https://github.com/itisha89/image_recreation/blob/main/gen_image_sdxl_2.jpg), [gen_image_sdxl_3.jpg](https://github.com/itisha89/image_recreation/blob/main/gen_image_sdxl_3.jpg)  
  - **Instruct-Pix2Pix** (Final Model): [generated_image_1](https://github.com/itisha89/image_recreation/blob/main/images_pix2pix_1.2.jpg), [generated_image_2](https://github.com/itisha89/image_recreation/blob/main/images_pix2pix_2.jpg), [generated_image_3](https://github.com/itisha89/image_recreation/blob/main/images_pix2pix_3.jpg).  
- **Final Decision**: The Instruct-Pix2Pix model generated the most accurate replica, making it the preferred model for final submission.  
